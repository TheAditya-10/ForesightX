# ForesightX Parameters Configuration
# =====================================
# Centralized configuration for ML pipeline stages
#
# This file contains all hyperparameters and settings for:
# 1. Data Ingestion (make_dataset.py) - Fetch stock data from Yahoo Finance
# 2. Data Preprocessing (preprocess.py) - Clean and split data into train/test
# 3. Feature Engineering (build_features.py) - Create technical indicators
# 4. Model Training - Train multiple ML/DL models
# 5. Model Evaluation - Evaluate and compare models
# 6. Logging & Storage - Configure logging and cloud storage
#
# Usage:
# - Edit parameters as needed for experimentation
# - All modules automatically load from this file
# - Keep this file in project root directory

# =============================================================================
# DATA PIPELINE CONFIGURATION
# =============================================================================

# Data Ingestion Configuration
data_ingestion:
  stock_symbol: AAPL
  start_date: '2015-01-01'
  end_date: '2023-12-31'
  source: yfinance  # Only yfinance is supported

# Data Preprocessing Configuration
preprocessing:
  handle_missing: ffill  # Options: ffill, bfill (uses forward fill then backward fill)
  remove_outliers: false  # Whether to remove outliers (false = detect only, true = remove)
  outlier_method: iqr  # Method to detect outliers: iqr or zscore
  outlier_threshold: 3.0  # IQR multiplier for outlier detection (3.0 = extreme outliers only)
  min_trading_days: 252  # Minimum trading days recommended for training

# Feature Engineering Configuration
feature_engineering:
  # Lag Features - Historical values shifted by N periods
  lag_periods: [1, 2, 3, 5, 10]  # Create lag features for these periods
  
  # Simple Moving Average (SMA) Windows
  sma_windows: [5, 10, 20, 50, 200]  # Standard SMA periods
  
  # Exponential Moving Average (EMA) Windows
  ema_windows: [12, 26, 50]  # Standard EMA periods for MACD and trend
  
  # RSI (Relative Strength Index) Configuration
  rsi_windows: [14, 21]  # RSI calculation windows
  rsi_oversold: 30  # Oversold threshold
  rsi_overbought: 70  # Overbought threshold
  
  # MACD (Moving Average Convergence Divergence) Configuration
  macd_fast: 12  # Fast EMA period
  macd_slow: 26  # Slow EMA period
  macd_signal: 9  # Signal line EMA period
  
  # Bollinger Bands Configuration
  bollinger_window: 20  # Rolling window for Bollinger Bands
  bollinger_std: 2  # Number of standard deviations
  
  # ATR (Average True Range) Configuration
  atr_windows: [14, 21]  # ATR calculation windows
  
  # Volatility Features Configuration
  volatility_windows: [5, 10, 20, 30]  # Rolling volatility windows
  
  # Volume Features Configuration
  volume_ma_windows: [5, 20, 50]  # Volume moving average windows
  vwap_window: 20  # VWAP calculation window
  
  # Rate of Change (ROC) Configuration
  roc_periods: [5, 10, 20]  # ROC calculation periods
  
  # Target Variables Configuration
  target_return_window: 1  # Predict next N days return (1 = next day)
  multiclass_bins: [-0.02, -0.005, 0.005, 0.02]  # Bins for multi-class target

# =============================================================================
# MODEL TRAINING CONFIGURATION
# =============================================================================

# Training Configuration
training:
  test_size: 0.2  # Proportion of data for testing (chronological split)
  validation_size: 0.1  # For model validation during training
  random_state: 42
  sequence_length: 60  # For deep learning models (number of historical days)
  batch_size: 32
  epochs: 100
  early_stopping_patience: 10
  learning_rate: 0.001

# Model Hyperparameters

# Classical Time Series Models
models:
  # ARIMA/SARIMAX
  arima:
    order: [5, 1, 0]
    seasonal_order: [1, 1, 1, 12]
    trend: 'ct'
  
  # Prophet
  prophet:
    changepoint_prior_scale: 0.05
    seasonality_prior_scale: 10.0
    seasonality_mode: 'multiplicative'
    yearly_seasonality: true
    weekly_seasonality: true
    daily_seasonality: false
  
  # XGBoost
  xgboost:
    n_estimators: 1000
    max_depth: 7
    learning_rate: 0.01
    subsample: 0.8
    colsample_bytree: 0.8
    gamma: 0.1
    min_child_weight: 3
    reg_alpha: 0.1
    reg_lambda: 1.0
    objective: 'reg:squarederror'
    eval_metric: 'rmse'
    early_stopping_rounds: 50
    verbosity: 0
  
  # LightGBM
  lightgbm:
    n_estimators: 1000
    num_leaves: 31
    max_depth: 7
    learning_rate: 0.01
    subsample: 0.8
    colsample_bytree: 0.8
    min_child_samples: 20
    reg_alpha: 0.1
    reg_lambda: 1.0
    objective: 'regression'
    metric: 'rmse'
    boosting_type: 'gbdt'
    verbose: -1
    early_stopping_rounds: 50
  
  # MLP (Multi-Layer Perceptron)
  mlp:
    hidden_layer_sizes: [128, 64, 32]  # Three hidden layers
    activation: 'relu'  # Options: relu, tanh, logistic
    solver: 'adam'  # Options: adam, sgd, lbfgs
    alpha: 0.0001  # L2 regularization parameter
    learning_rate: 'adaptive'  # Options: constant, invscaling, adaptive
    learning_rate_init: 0.001  # Initial learning rate
    max_iter: 200  # Maximum number of iterations
    early_stopping: true  # Enable early stopping
    validation_fraction: 0.1  # Fraction of training data for validation
    n_iter_no_change: 15  # Iterations with no improvement before stopping
    random_state: 42
  
  # LSTM (PyTorch)
  lstm:
    input_size: 20  # Number of features
    hidden_size: 128
    num_layers: 2
    dropout: 0.2
    bidirectional: false
    output_size: 1
    learning_rate: 0.001
    weight_decay: 0.0001
    optimizer: 'adam'
    scheduler: 'ReduceLROnPlateau'
    scheduler_params:
      mode: 'min'
      factor: 0.5
      patience: 5
      min_lr: 0.00001
  
  # GRU (PyTorch)
  gru:
    input_size: 20
    hidden_size: 128
    num_layers: 2
    dropout: 0.2
    bidirectional: false
    output_size: 1
    learning_rate: 0.001
    weight_decay: 0.0001
    optimizer: 'adam'
    scheduler: 'ReduceLROnPlateau'
    scheduler_params:
      mode: 'min'
      factor: 0.5
      patience: 5
      min_lr: 0.00001
  
  # Transformer (PyTorch)
  transformer:
    input_size: 20
    d_model: 128
    nhead: 8
    num_encoder_layers: 3
    dim_feedforward: 512
    dropout: 0.2
    output_size: 1
    learning_rate: 0.0001
    weight_decay: 0.0001
    optimizer: 'adam'
    scheduler: 'ReduceLROnPlateau'
    scheduler_params:
      mode: 'min'
      factor: 0.5
      patience: 5
      min_lr: 0.00001

# Ensemble Configuration
ensemble:
  method: 'weighted_average'  # Options: weighted_average, stacking, voting
  top_n_models: 5  # Select top N models based on validation performance
  weights: 'auto'  # Options: auto (based on performance), equal, custom
  custom_weights:
    lstm: 0.25
    gru: 0.25
    transformer: 0.15
    xgboost: 0.15
    lightgbm: 0.10
    mlp: 0.10
  stacking_meta_model: 'ridge'  # For stacking ensemble

# =============================================================================
# EVALUATION & MONITORING CONFIGURATION
# =============================================================================

# Evaluation Configuration
evaluation:
  metrics:
    - RMSE
    - MAE
    - MAPE
    - R2
    - Sharpe_Ratio
    - Directional_Accuracy
    - Max_Drawdown
  
  plot_predictions: true
  save_predictions: true
  confidence_intervals: true
  confidence_level: 0.95
  
  # Financial Metrics
  financial_metrics:
    risk_free_rate: 0.02  # 2% annual risk-free rate
    trading_cost: 0.001  # 0.1% per trade
    initial_capital: 100000

# =============================================================================
# INFRASTRUCTURE CONFIGURATION
# =============================================================================

# Logging Configuration
logging:
  level: INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: '%(asctime)s | %(name)s | %(levelname)s | %(message)s'
  save_to_file: true
  log_dir: logs
  max_bytes: 10485760  # 10MB per log file
  backup_count: 5  # Number of backup log files to keep

# DVC (Data Version Control) Configuration
# Currently using local storage in /data directory
# DVC tracks data versions locally without requiring cloud storage
dvc:
  local_storage: true  # Using local storage for now
  remote_storage: false  # Will enable S3 remote in production
  cache_dir: .dvc/cache  # Local DVC cache directory

# AWS S3 Configuration (Optional - for production deployment)
# Currently NOT required - all data stored locally in /data directory
# S3 will be configured later when moving to production
# To enable: Set credentials in .env file and set remote_storage: true above
s3:
  enabled: false  # Set to true when S3 is configured
  bucket_name: foresightx-mlops  # Your S3 bucket name (configure later)
  save_to_s3: false  # Whether to upload data to S3
  paths:
    raw_data: 'raw-data/'
    processed_data: 'processed-data/'
    features: 'features/'  # Feature-engineered data
    models: 'models/'
    predictions: 'predictions/'
    reports: 'reports/'
  
  # Model versioning
  versioning:
    enabled: true
    strategy: 'timestamp'  # Options: timestamp, semantic, hash

# Model Registry Configuration
model_registry:
  track_experiments: true
  save_artifacts: true
  compare_metrics: true
  auto_archive: true
  retention_days: 90
  
  # Default stage for newly registered models
  # Options: None, "Staging", "Production", "Archived"
  # None = No stage assignment (model needs manual promotion)
  # Staging = For models under testing/validation
  # Production = For models actively serving predictions
  # Archived = For deprecated/old models
  default_stage: "Staging"  # Recommended: Start with Staging, promote to Production after validation

# MLflow & DagsHub Configuration
# DagsHub provides hosted MLflow tracking server
# Set DAGSHUB_TOKEN environment variable for authentication
mlflow:
  enabled: true  # Set to false to disable MLflow tracking
  dagshub_username: 'TheAditya-10'  # Your DagsHub username
  dagshub_repo: 'ForesightX'  # Your DagsHub repository name
  experiment_name: 'MLP_Stock_Prediction'  # MLflow experiment name
  
  # What to log
  log_params: true  # Log hyperparameters
  log_metrics: true  # Log evaluation metrics
  log_models: true  # Log model artifacts
  log_artifacts: true  # Log additional files (plots, configs)
  
  # Auto-logging
  autolog: false  # Disable auto-logging (we do manual logging for better control)